{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06157cf5-3c0e-4edc-830f-863b3a5bdc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 55, 55]          23,296\n",
      "              ReLU-2           [-1, 64, 55, 55]               0\n",
      "         MaxPool2d-3           [-1, 64, 27, 27]               0\n",
      "            Conv2d-4          [-1, 192, 27, 27]         307,392\n",
      "              ReLU-5          [-1, 192, 27, 27]               0\n",
      "         MaxPool2d-6          [-1, 192, 13, 13]               0\n",
      "            Conv2d-7          [-1, 384, 13, 13]         663,936\n",
      "              ReLU-8          [-1, 384, 13, 13]               0\n",
      "            Conv2d-9          [-1, 256, 13, 13]         884,992\n",
      "             ReLU-10          [-1, 256, 13, 13]               0\n",
      "           Conv2d-11          [-1, 256, 13, 13]         590,080\n",
      "             ReLU-12          [-1, 256, 13, 13]               0\n",
      "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
      "AdaptiveAvgPool2d-14            [-1, 256, 6, 6]               0\n",
      "          Dropout-15                 [-1, 9216]               0\n",
      "           Linear-16                 [-1, 4096]      37,752,832\n",
      "             ReLU-17                 [-1, 4096]               0\n",
      "          Dropout-18                 [-1, 4096]               0\n",
      "           Linear-19                 [-1, 4096]      16,781,312\n",
      "             ReLU-20                 [-1, 4096]               0\n",
      "           Linear-21                    [-1, 3]          12,291\n",
      "       LogSoftmax-22                    [-1, 3]               0\n",
      "================================================================\n",
      "Total params: 57,016,131\n",
      "Trainable params: 12,291\n",
      "Non-trainable params: 57,003,840\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 8.37\n",
      "Params size (MB): 217.50\n",
      "Estimated Total Size (MB): 226.44\n",
      "----------------------------------------------------------------\n",
      "Epoch: 1/10\n",
      "Epoch : 001, Training: Loss: 0.9843, Accuracy: 54.7558%, \n",
      "\t\tValidation : Loss : 0.8110, Accuracy: 69.3333%, Time: 26.3193s\n",
      "Epoch: 2/10\n",
      "Epoch : 002, Training: Loss: 0.7669, Accuracy: 68.8946%, \n",
      "\t\tValidation : Loss : 0.6571, Accuracy: 74.6667%, Time: 26.3066s\n",
      "Epoch: 3/10\n",
      "Epoch : 003, Training: Loss: 0.7167, Accuracy: 69.4087%, \n",
      "\t\tValidation : Loss : 0.7361, Accuracy: 70.6667%, Time: 28.6000s\n",
      "Epoch: 4/10\n",
      "Epoch : 004, Training: Loss: 0.6982, Accuracy: 73.0077%, \n",
      "\t\tValidation : Loss : 0.6615, Accuracy: 76.0000%, Time: 25.2738s\n",
      "Epoch: 5/10\n",
      "Epoch : 005, Training: Loss: 0.6622, Accuracy: 72.7506%, \n",
      "\t\tValidation : Loss : 0.5797, Accuracy: 80.0000%, Time: 28.4736s\n",
      "Epoch: 6/10\n",
      "Epoch : 006, Training: Loss: 0.6437, Accuracy: 74.2931%, \n",
      "\t\tValidation : Loss : 0.6621, Accuracy: 80.0000%, Time: 22.4767s\n",
      "Epoch: 7/10\n",
      "Epoch : 007, Training: Loss: 0.6349, Accuracy: 75.5784%, \n",
      "\t\tValidation : Loss : 0.6685, Accuracy: 74.6667%, Time: 24.7596s\n",
      "Epoch: 8/10\n",
      "Epoch : 008, Training: Loss: 0.6293, Accuracy: 74.0360%, \n",
      "\t\tValidation : Loss : 0.5954, Accuracy: 74.6667%, Time: 22.1573s\n",
      "Epoch: 9/10\n",
      "Epoch : 009, Training: Loss: 0.6155, Accuracy: 73.7789%, \n",
      "\t\tValidation : Loss : 0.6049, Accuracy: 74.6667%, Time: 22.9998s\n",
      "Epoch: 10/10\n",
      "Epoch : 010, Training: Loss: 0.6231, Accuracy: 73.7789%, \n",
      "\t\tValidation : Loss : 0.5544, Accuracy: 77.3333%, Time: 22.0064s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import scipy.io\n",
    "import pandas\n",
    "from scipy.io import loadmat \n",
    "import torch, torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import time \n",
    "import math\n",
    "import random\n",
    "from scipy.fftpack import dct, idct # import for discrete cosine transform\n",
    "from torchsummary import summary \n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "matPath = \".../method_1_data_augmentation_final.mat\"\n",
    "data = scipy.io.loadmat(matPath) \n",
    " \n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, mat_path, transform=None, train=True, fold=1):\n",
    "        self.mat_data = scipy.io.loadmat(mat_path)\n",
    "        self.fold = fold\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.train_indices = self.mat_data['shuffle'][0][:389] - 1\n",
    "        self.test_indices = self.mat_data['shuffle'][0][389:464] - 1\n",
    "        self.y_train = self.mat_data['labels'][0][self.train_indices]\n",
    "        self.y_test = self.mat_data['labels'][0][self.test_indices]\n",
    "        self.num_classes = len(np.unique(self.y_train))\n",
    "        self.images = self.mat_data['images'].astype(np.uint8)    # contains 383 images with images being of size (312,417,3)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_indices)\n",
    "        else:\n",
    "            return len(self.test_indices)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            img = Image.fromarray(self.images[self.train_indices[idx]])\n",
    "            label = self.y_train[idx] - 1  # shift the labels to start from 0\n",
    "        else:\n",
    "            img = Image.fromarray(self.images[self.test_indices[idx]])\n",
    "            label = self.y_test[idx] - 1   # shift the labels to start from 0\n",
    "            \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "\n",
    "\n",
    "# load dataset\n",
    "data = scipy.io.loadmat(matPath)\n",
    "DIV = data['shuffle'][0]   # Division between training and test set\n",
    "DIM1 = 389  # Number of training patterns\n",
    "DIM2 = 464 # Number of patterns\n",
    "NF = 5 \n",
    "\n",
    "# # Neural network parameters\n",
    "miniBatchSize = 30 \n",
    "num_classes = 3\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=227, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize(size=256),\n",
    "    transforms.CenterCrop(size=227),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = MyDataset(mat_path=matPath, transform=train_transforms, train=True, fold=1)\n",
    "valid_dataset = MyDataset(mat_path=matPath, transform=val_transforms, train=False, fold=1)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=miniBatchSize, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=miniBatchSize, shuffle=False)\n",
    "\n",
    "train_data_size = DIV[:DIM1].shape[0]\n",
    "valid_data_size = DIV[DIM1:DIM2].shape[0]\n",
    "\n",
    "# Transer learning model\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "alexnet\n",
    "\n",
    "# Freeze model parameters : Only use last layer and not update whole params (faster)\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False\n",
    "# Change the final layer of AlexNet Model for Transfer Learning\n",
    "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
    "alexnet.classifier.add_module(\"7\", nn.LogSoftmax(dim = 1))\n",
    "alexnet\n",
    "summary(alexnet, (3, 224, 224))\n",
    "\n",
    "# Define Optimizer and Loss Function\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.Adam(alexnet.parameters())\n",
    "optimizer\n",
    "\n",
    "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
    "    '''\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "  \n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        \n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "        \n",
    "        for i, (inputs, labels) in enumerate(train_data_loader):\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs) \n",
    "               \n",
    "            # print(labels.long())\n",
    "            # print(outputs)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_criterion(outputs, labels.long())\n",
    "            \n",
    "            # Backpropagate the gradients\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Compute the accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            \n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            \n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "            \n",
    "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "\n",
    "            \n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Set to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            # Validation loop\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass - compute outputs on input data using the model\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_criterion(outputs, labels.long())\n",
    "\n",
    "                # Compute the total loss for the batch and add it to valid_loss\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                ret, predictions = torch.max(outputs.data, 1)\n",
    "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "\n",
    "                # Convert correct_counts to float and then compute the mean\n",
    "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "\n",
    "                # Compute total accuracy in the whole batch and add to valid_acc\n",
    "                valid_acc += acc.item() * inputs.size(0)\n",
    "\n",
    "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "            \n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/train_data_size \n",
    "        avg_train_acc = train_acc/train_data_size\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss/valid_data_size \n",
    "        avg_valid_acc = valid_acc/valid_data_size\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "                \n",
    "        epoch_end = time.time()\n",
    "    \n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch+1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))\n",
    "        \n",
    "        # Save if the model has best accuracy till now\n",
    "        #torch.save(model, dataset+'_model_'+str(epoch)+'.pt')\n",
    "            \n",
    "    return model, history\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "trained_model, history = train_and_validate(alexnet, loss_func, optimizer, num_epochs)\n",
    "\n",
    "torch.save(history, '.../DCT_version_1_TRAIN_History_10_epochs.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed6e8ae-6d38-47d8-adaf-8b0c58f4c312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
